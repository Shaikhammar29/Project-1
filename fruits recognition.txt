!wget https://www.dropbox.com/s/fxn3ldztzwxm0rw/FruitsData.zip

!unzip -q "/content/FruitsData.zip"

# Imports Required for this project
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

tf.random.set_seed(4)

# Creating the Pathlib PATH objects
train_path = Path("fruits-360/Training")
test_path = Path("fruits-360/Test")

# Getting Image paths
train_image_paths = list(train_path.glob("*/*"))
train_image_paths = list(map(lambda x : str(x) , train_image_paths))

train_image_paths[:10]

# Getting there respective labels

def get_label(image_path):
    return image_path.split("/")[-2]

train_image_labels = list(map(lambda x : get_label(x) , train_image_paths))
train_image_labels[:10]

from sklearn.preprocessing import LabelEncoder

Le =  LabelEncoder()
train_image_labels = Le.fit_transform(train_image_labels)

train_image_labels[:10]

train_image_labels = tf.keras.utils.to_categorical(train_image_labels)

train_image_labels[:10]

from sklearn.model_selection import train_test_split

Train_paths , Val_paths , Train_labels , Val_labels = train_test_split(train_image_paths , train_image_labels)

Train_paths[:10] , Train_labels[:10]

# Function used for Transformation

def load(image , label):
    image = tf.io.read_file(image)
    image = tf.io.decode_jpeg(image , channels = 3)
    return image , label

# Define IMAGE SIZE and BATCH SIZE
IMG_SIZE = 224
BATCH_SIZE = 32

# Basic Transformation
resize = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE , IMG_SIZE)
])

# Data Augmentation
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal"),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
    tf.keras.layers.experimental.preprocessing.RandomZoom(height_factor = (-0.3 , -0.2))
])

# Function used to create a Tensorflow Data Object
AUTOTUNE = tf.data.experimental.AUTOTUNE
def get_dataset(paths , labels , train = True):
    image_paths = tf.convert_to_tensor(paths)
    labels = tf.convert_to_tensor(labels)

    image_dataset = tf.data.Dataset.from_tensor_slices(image_paths)
    label_dataset = tf.data.Dataset.from_tensor_slices(labels)

    dataset = tf.data.Dataset.zip((image_dataset , label_dataset))

    dataset = dataset.map(lambda image , label : load(image , label))
    dataset = dataset.map(lambda image , label : (resize(image) , label) , num_parallel_calls = AUTOTUNE)
    dataset = dataset.shuffle(1000)
    dataset = dataset.batch(BATCH_SIZE)

    if train:
        dataset = dataset.map(lambda image , label : (data_augmentation(image) , label) , num_parallel_calls = AUTOTUNE)

    dataset = dataset.repeat()
    return dataset

# Creating Train dataset objects and Verifying it
train_dataset = get_dataset(Train_paths, Train_labels)

# Get the first image and label from the dataset
image, label = next(iter(train_dataset))

# Print the shape of the image and label
print(image.shape)
print(label.shape)

# View a sample training Image
print(Le.inverse_transform(np.argmax(label , axis = 1))[0])
plt.imshow((image[0].numpy()/255).reshape(224 , 224 , 3))

val_dataset = get_dataset(Val_paths, Val_labels, train=False)

# Get the first image and label from the dataset
image, label = next(iter(val_dataset))

# Print the shape of the image and label
print(image.shape)
print(label.shape)

# Building EfficientNetB3 model
from tensorflow.keras.applications import EfficientNetB3

backbone = EfficientNetB3(
    input_shape = (224, 224, 3),
    include_top = False
)

model = tf.keras.Sequential([
    backbone,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(131 , activation='softmax')
])

model.summary()

# Compiling your model by providing the Optimizer , Loss and Metrics
model.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-07),
    loss = 'categorical_crossentropy',
    metrics = ['accuracy' , tf.keras.metrics.Precision(name = 'precision'), tf.keras.metrics.Recall(name = 'recall')]
)

# Train the model
history = model.fit(
    train_dataset,
    steps_per_epoch = len(Train_paths)//BATCH_SIZE,
    epochs = 1,
    # callbacks = [checkpoint , early_stop],
    validation_data = val_dataset,
    validation_steps = len(Val_paths)//BATCH_SIZE,
)

# Turn off the backbone
model.layers[0].trainable = False

# Defining our callbacks
checkpoint = tf.keras.callbacks.ModelCheckpoint("best_weights.h5", verbose = 1, save_best_only = True, save_weights_only = True)
early_stop = tf.keras.callbacks.EarlyStopping(patience = 4)

model.summary()

# Train the phase 2 of model
history = model.fit(
    train_dataset,
    steps_per_epoch = len(Train_paths)//BATCH_SIZE,
    epochs = 8,
    callbacks = [checkpoint , early_stop],
    validation_data = val_dataset,
    validation_steps = len(Val_paths)//BATCH_SIZE,
)

from tensorflow.keras.applications import EfficientNetB3

backbone = EfficientNetB3(
    input_shape = (224, 224, 3),
    include_top = False
)

model = tf.keras.Sequential([
    backbone,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(128 , activation = 'relu'),
    tf.keras.layers.Dense(131 , activation = 'softmax')
])

model.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001 , beta_1 = 0.9 , beta_2 = 0.999 , epsilon = 1e-07),
    loss = 'categorical_crossentropy',
    metrics = ['accuracy' , tf.keras.metrics.Precision(name = 'precision') , tf.keras.metrics.Recall(name = 'recall')]
)

model.load_weights("best_weights.h5", by_name=True)

# Create Dataset Object for 'Testing' Set just the way we did for Training and Validation
test_image_paths = list(test_path.glob("*/*"))
test_image_paths = list(map(lambda x : str(x) , test_image_paths))
test_labels = list(map(lambda x : get_label(x) , test_image_paths))

test_labels = Le.transform(test_labels)
test_labels = tf.keras.utils.to_categorical(test_labels)

test_image_paths = tf.convert_to_tensor(test_image_paths)
test_labels = tf.convert_to_tensor(test_labels)

def decode_image(image , label):
    image = tf.io.read_file(image)
    image = tf.io.decode_jpeg(image , channels = 3)
    image = tf.image.resize(image , [224 , 224] , method = "bilinear")
    return image , label

test_dataset = (
     tf.data.Dataset
    .from_tensor_slices((test_image_paths , test_labels))
    .map(decode_image)
    .batch(BATCH_SIZE)
)

# Verify Test Dataset Objects
image , label = next(iter(test_dataset))
print(image.shape)
print(label.shape)

# View a sample Validation Image
print(Le.inverse_transform(np.argmax(label , axis = 1))[0])
plt.imshow((image[0].numpy()/255).reshape(224 , 224 , 3))

# Evaluating the loaded model
loss , acc , prec , rec = model.evaluate(test_dataset)

print(" Testing Acc : " , acc)
print(" Testing Precision " , prec)
print(" Testing Recall " , rec)

def LoadImage(image_path):
    image = tf.io.read_file(image_path)
    image = tf.io.decode_jpeg(image , channels = 3)
    image = tf.image.resize(image , [224 , 224] , method = "bilinear")
    plt.imshow(image.numpy()/255)
    image = tf.expand_dims(image , 0)
    return image

def RealtimePrediction(image_path , model , encoder_):
    image = LoadImage(image_path)
    prediction = model.predict(image)
    prediction = np.argmax(prediction , axis = 1)
    return encoder_.inverse_transform(prediction)[0]
 

RealtimePrediction("/content/fruits-360/Training/Pear Williams/100_100.jpg" , model , Le)

