!nvidia-smi

!wget http://www.dropbox.com/s/tlxserrdhe240lu/archive.zip

# Unzipping the Data
!unzip -q "archive.zip"

# Imports Required by this project
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

tf.random.set_seed(4)

# Creating the Pathlib PATH objects
train_path = Path("chest_xray/train/")
validation_path = Path("chest_xray/test/")
test_path = Path("chest_xray/val/")


# Collecting all the Paths Inside "Normal" and "Pneumonia" folders of the above paths
train_image_paths = list(train_path.glob("*/*"))
val_image_paths = list(validation_path.glob("*/*"))

print(train_image_paths[:3])

# Convert Posix paths to normal strings
train_image_paths = list(map(lambda x: str(x), train_image_paths))
val_image_paths = list(map(lambda x: str(x), val_image_paths))

print(train_image_paths[:3])


# Collect Length for Training and Validation Datasets
train_dataset_length = len(train_image_paths)
val_dataset_length = len(val_image_paths)

# Every Image has Label in its path, so let's slice it
LABELS = {'NORMAL': 0, 'PNEUMONIA': 1}
INV_LABELS = {0: 'NORMAL', 1: 'PNEUMONIA'}

def get_label(path: str) -> int:
    return LABELS[path.split("/")[-2]]

train_labels = list(map(lambda x: get_label(x), train_image_paths))
val_labels = list(map(lambda x: get_label(x), val_image_paths))

print(train_labels[:3])

# Function used to create a TensorFlow Dataset object
def get_dataset(paths, labels, train=True):
    BATCH_SIZE = 32

    image_dataset = tf.data.Dataset.from_tensor_slices(paths)
    label_dataset = tf.data.Dataset.from_tensor_slices(labels)

    dataset = tf.data.Dataset.zip((image_dataset, label_dataset)).shuffle(1000)

    dataset = dataset.map(lambda image, label: load_and_transform(image, label, train))
    dataset = dataset.repeat()
    dataset = dataset.shuffle(2048)
    dataset = dataset.batch(BATCH_SIZE)

    return dataset


# Now we have all training, validation image paths, and their respective labels

def load_and_transform(image, label, train=True):
    image = tf.io.read_file(image)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [224, 224], method="nearest")
    if train:
        image = tf.image.random_flip_left_right(image)
    return image, label

# View a sample Train Image
train_dataset = get_dataset(train_image_paths, train_labels)
image, label = next(iter(train_dataset))

label_name = INV_LABELS[label[0].numpy()]
print(label_name)

plt.imshow(image[0].numpy().reshape(224, 224, 3))
plt.show()

%time val_dataset = get_dataset(val_image_paths , val_labels , train = False)

image , label = next(iter(val_dataset))
print(image.shape)
print(label.shape)

#Building ResNet50 Model
import tensorflow as tf
from tensorflow.keras.applications import ResNet50V2

backbone = ResNet50V2(
    input_shape=(224, 224, 3),
    include_top=False
)
model = tf.keras.Sequential([
    backbone,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.summary()

# Compiling your Model by providing the Optimizer , Loss and Metrics
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),
    loss = 'binary_crossentropy',
    metrics = ['accuracy', tf.keras.metrics.Precision(name='precision'),tf.keras.metrics.Recall(name='recall')]
)

# Defining our callbacks
checkpoint = tf.keras.callbacks.ModelCheckpoint("best_weights.h5",verbose=1, save_best_only=True, save_weights_only=True)
early_stop = tf.keras.callbacks.EarlyStopping(patience=4)

# Define BATCH_SIZE
BATCH_SIZE = 32

# Train the model
history = model.fit(
    train_dataset,
    steps_per_epoch=train_dataset_length // BATCH_SIZE,
    epochs=8,
    callbacks=[checkpoint, early_stop],
    validation_data=val_dataset,
    validation_steps=val_dataset_length // BATCH_SIZE
)

# Interpreting the Metrics
fig, ax =  plt.subplots(1, 4, figsize=(20, 3))
ax = ax.ravel()

for i, met in enumerate(['precision', 'recall', 'accuracy', 'loss']):
    ax[i].plot(history.history[met])
    ax[i].plot(history.history['val_' + met])
    ax[i].set_title('Model {}'.format(met))
    ax[i].set_xlabel('epochs')
    ax[i].set_ylabel(met)
    ax[i].legend(['train', 'val'])

# Saving the best Model

# Load the best weights
model.load_weights("best_weights.h5")
# Save the whole Model (weights + architecture)
model.save("model.h5")

# Loading the whole model
loaded_model = tf.keras.models.load_model("model.h5")

# Create a Dataset object for 'Testing' Set just the way we did for Training and Validation
test_image_paths = list(test_path.glob("*/*"))
test_image_paths = list(map(lambda x : str(x) , test_image_paths))
test_labels = list(map(lambda x : get_label(x) , test_image_paths))

test_image_paths = tf.convert_to_tensor(test_image_paths)
test_labels = tf.convert_to_tensor(test_labels)

def decode_image(image , label):
    image = tf.io.read_file(image)
    image = tf.io.decode_jpeg(image , channels = 3)
    image = tf.image.resize(image , [224 , 224] , method='nearest')
    return image , label

test_dataset = (
    tf.data.Dataset
   .from_tensor_slices((test_image_paths , test_labels))
   .map(decode_image)
   .batch(BATCH_SIZE)
)

# Verify Test Dataset Object
image , label = next(iter(test_dataset))
print(image.shape)
print(label.shape)

# View a sample Validation Image
print(INV_LABELS[label[0].numpy()])
plt.imshow(image[0].numpy().reshape(224 , 224 , 3))

# Evaluating the Loaded Model
loss, acc, prec, rec =  loaded_model.evaluate(test_dataset)

print("Training Acc : " , acc)
print("Training Precision " , prec)
print("Training Recall " , rec)

